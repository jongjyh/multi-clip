{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\"EVA_AltCLIP_g_14\",\"/sharefs/baai-mrnd/sunquan/eva_clip_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['text.proj', 'text.transformer.roberta.embeddings.position_ids', 'text.transformer.roberta.embeddings.word_embeddings.weight', 'text.transformer.roberta.embeddings.position_embeddings.weight', 'text.transformer.roberta.embeddings.token_type_embeddings.weight', 'text.transformer.roberta.embeddings.LayerNorm.weight', 'text.transformer.roberta.embeddings.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.0.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.0.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.0.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.0.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.0.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.0.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.0.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.0.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.0.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.0.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.0.output.dense.weight', 'text.transformer.roberta.encoder.layer.0.output.dense.bias', 'text.transformer.roberta.encoder.layer.0.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.0.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.1.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.1.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.1.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.1.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.1.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.1.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.1.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.1.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.1.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.1.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.1.output.dense.weight', 'text.transformer.roberta.encoder.layer.1.output.dense.bias', 'text.transformer.roberta.encoder.layer.1.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.1.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.2.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.2.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.2.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.2.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.2.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.2.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.2.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.2.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.2.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.2.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.2.output.dense.weight', 'text.transformer.roberta.encoder.layer.2.output.dense.bias', 'text.transformer.roberta.encoder.layer.2.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.2.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.3.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.3.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.3.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.3.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.3.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.3.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.3.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.3.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.3.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.3.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.3.output.dense.weight', 'text.transformer.roberta.encoder.layer.3.output.dense.bias', 'text.transformer.roberta.encoder.layer.3.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.3.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.4.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.4.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.4.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.4.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.4.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.4.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.4.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.4.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.4.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.4.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.4.output.dense.weight', 'text.transformer.roberta.encoder.layer.4.output.dense.bias', 'text.transformer.roberta.encoder.layer.4.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.4.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.5.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.5.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.5.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.5.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.5.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.5.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.5.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.5.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.5.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.5.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.5.output.dense.weight', 'text.transformer.roberta.encoder.layer.5.output.dense.bias', 'text.transformer.roberta.encoder.layer.5.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.5.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.6.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.6.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.6.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.6.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.6.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.6.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.6.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.6.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.6.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.6.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.6.output.dense.weight', 'text.transformer.roberta.encoder.layer.6.output.dense.bias', 'text.transformer.roberta.encoder.layer.6.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.6.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.7.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.7.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.7.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.7.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.7.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.7.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.7.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.7.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.7.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.7.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.7.output.dense.weight', 'text.transformer.roberta.encoder.layer.7.output.dense.bias', 'text.transformer.roberta.encoder.layer.7.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.7.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.8.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.8.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.8.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.8.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.8.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.8.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.8.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.8.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.8.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.8.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.8.output.dense.weight', 'text.transformer.roberta.encoder.layer.8.output.dense.bias', 'text.transformer.roberta.encoder.layer.8.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.8.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.9.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.9.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.9.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.9.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.9.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.9.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.9.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.9.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.9.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.9.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.9.output.dense.weight', 'text.transformer.roberta.encoder.layer.9.output.dense.bias', 'text.transformer.roberta.encoder.layer.9.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.9.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.10.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.10.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.10.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.10.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.10.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.10.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.10.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.10.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.10.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.10.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.10.output.dense.weight', 'text.transformer.roberta.encoder.layer.10.output.dense.bias', 'text.transformer.roberta.encoder.layer.10.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.10.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.11.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.11.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.11.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.11.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.11.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.11.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.11.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.11.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.11.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.11.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.11.output.dense.weight', 'text.transformer.roberta.encoder.layer.11.output.dense.bias', 'text.transformer.roberta.encoder.layer.11.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.11.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.12.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.12.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.12.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.12.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.12.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.12.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.12.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.12.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.12.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.12.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.12.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.12.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.12.output.dense.weight', 'text.transformer.roberta.encoder.layer.12.output.dense.bias', 'text.transformer.roberta.encoder.layer.12.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.12.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.13.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.13.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.13.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.13.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.13.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.13.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.13.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.13.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.13.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.13.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.13.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.13.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.13.output.dense.weight', 'text.transformer.roberta.encoder.layer.13.output.dense.bias', 'text.transformer.roberta.encoder.layer.13.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.13.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.14.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.14.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.14.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.14.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.14.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.14.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.14.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.14.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.14.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.14.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.14.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.14.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.14.output.dense.weight', 'text.transformer.roberta.encoder.layer.14.output.dense.bias', 'text.transformer.roberta.encoder.layer.14.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.14.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.15.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.15.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.15.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.15.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.15.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.15.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.15.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.15.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.15.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.15.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.15.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.15.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.15.output.dense.weight', 'text.transformer.roberta.encoder.layer.15.output.dense.bias', 'text.transformer.roberta.encoder.layer.15.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.15.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.16.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.16.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.16.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.16.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.16.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.16.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.16.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.16.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.16.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.16.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.16.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.16.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.16.output.dense.weight', 'text.transformer.roberta.encoder.layer.16.output.dense.bias', 'text.transformer.roberta.encoder.layer.16.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.16.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.17.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.17.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.17.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.17.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.17.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.17.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.17.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.17.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.17.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.17.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.17.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.17.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.17.output.dense.weight', 'text.transformer.roberta.encoder.layer.17.output.dense.bias', 'text.transformer.roberta.encoder.layer.17.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.17.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.18.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.18.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.18.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.18.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.18.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.18.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.18.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.18.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.18.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.18.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.18.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.18.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.18.output.dense.weight', 'text.transformer.roberta.encoder.layer.18.output.dense.bias', 'text.transformer.roberta.encoder.layer.18.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.18.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.19.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.19.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.19.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.19.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.19.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.19.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.19.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.19.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.19.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.19.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.19.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.19.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.19.output.dense.weight', 'text.transformer.roberta.encoder.layer.19.output.dense.bias', 'text.transformer.roberta.encoder.layer.19.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.19.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.20.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.20.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.20.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.20.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.20.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.20.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.20.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.20.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.20.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.20.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.20.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.20.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.20.output.dense.weight', 'text.transformer.roberta.encoder.layer.20.output.dense.bias', 'text.transformer.roberta.encoder.layer.20.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.20.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.21.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.21.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.21.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.21.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.21.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.21.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.21.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.21.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.21.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.21.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.21.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.21.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.21.output.dense.weight', 'text.transformer.roberta.encoder.layer.21.output.dense.bias', 'text.transformer.roberta.encoder.layer.21.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.21.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.22.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.22.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.22.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.22.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.22.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.22.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.22.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.22.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.22.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.22.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.22.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.22.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.22.output.dense.weight', 'text.transformer.roberta.encoder.layer.22.output.dense.bias', 'text.transformer.roberta.encoder.layer.22.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.22.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.23.attention.self.query.weight', 'text.transformer.roberta.encoder.layer.23.attention.self.query.bias', 'text.transformer.roberta.encoder.layer.23.attention.self.key.weight', 'text.transformer.roberta.encoder.layer.23.attention.self.key.bias', 'text.transformer.roberta.encoder.layer.23.attention.self.value.weight', 'text.transformer.roberta.encoder.layer.23.attention.self.value.bias', 'text.transformer.roberta.encoder.layer.23.attention.output.dense.weight', 'text.transformer.roberta.encoder.layer.23.attention.output.dense.bias', 'text.transformer.roberta.encoder.layer.23.attention.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.23.attention.output.LayerNorm.bias', 'text.transformer.roberta.encoder.layer.23.intermediate.dense.weight', 'text.transformer.roberta.encoder.layer.23.intermediate.dense.bias', 'text.transformer.roberta.encoder.layer.23.output.dense.weight', 'text.transformer.roberta.encoder.layer.23.output.dense.bias', 'text.transformer.roberta.encoder.layer.23.output.LayerNorm.weight', 'text.transformer.roberta.encoder.layer.23.output.LayerNorm.bias', 'text.transformer.roberta.pooler.dense.weight', 'text.transformer.roberta.pooler.dense.bias', 'text.transformer.transformation.weight', 'text.transformer.transformation.bias', 'text.transformer.pre_LN.weight', 'text.transformer.pre_LN.bias'], unexpected_keys=['text.positional_embedding', 'text.text_projection', 'text.logit_scale', 'text.token_embedding.weight', 'text.ln_final.weight', 'text.ln_final.bias', 'text.transformer.resblocks.0.ln_1.weight', 'text.transformer.resblocks.0.ln_1.bias', 'text.transformer.resblocks.0.attn.in_proj_weight', 'text.transformer.resblocks.0.attn.in_proj_bias', 'text.transformer.resblocks.0.attn.out_proj.weight', 'text.transformer.resblocks.0.attn.out_proj.bias', 'text.transformer.resblocks.0.ln_2.weight', 'text.transformer.resblocks.0.ln_2.bias', 'text.transformer.resblocks.0.mlp.c_fc.weight', 'text.transformer.resblocks.0.mlp.c_fc.bias', 'text.transformer.resblocks.0.mlp.c_proj.weight', 'text.transformer.resblocks.0.mlp.c_proj.bias', 'text.transformer.resblocks.1.ln_1.weight', 'text.transformer.resblocks.1.ln_1.bias', 'text.transformer.resblocks.1.attn.in_proj_weight', 'text.transformer.resblocks.1.attn.in_proj_bias', 'text.transformer.resblocks.1.attn.out_proj.weight', 'text.transformer.resblocks.1.attn.out_proj.bias', 'text.transformer.resblocks.1.ln_2.weight', 'text.transformer.resblocks.1.ln_2.bias', 'text.transformer.resblocks.1.mlp.c_fc.weight', 'text.transformer.resblocks.1.mlp.c_fc.bias', 'text.transformer.resblocks.1.mlp.c_proj.weight', 'text.transformer.resblocks.1.mlp.c_proj.bias', 'text.transformer.resblocks.2.ln_1.weight', 'text.transformer.resblocks.2.ln_1.bias', 'text.transformer.resblocks.2.attn.in_proj_weight', 'text.transformer.resblocks.2.attn.in_proj_bias', 'text.transformer.resblocks.2.attn.out_proj.weight', 'text.transformer.resblocks.2.attn.out_proj.bias', 'text.transformer.resblocks.2.ln_2.weight', 'text.transformer.resblocks.2.ln_2.bias', 'text.transformer.resblocks.2.mlp.c_fc.weight', 'text.transformer.resblocks.2.mlp.c_fc.bias', 'text.transformer.resblocks.2.mlp.c_proj.weight', 'text.transformer.resblocks.2.mlp.c_proj.bias', 'text.transformer.resblocks.3.ln_1.weight', 'text.transformer.resblocks.3.ln_1.bias', 'text.transformer.resblocks.3.attn.in_proj_weight', 'text.transformer.resblocks.3.attn.in_proj_bias', 'text.transformer.resblocks.3.attn.out_proj.weight', 'text.transformer.resblocks.3.attn.out_proj.bias', 'text.transformer.resblocks.3.ln_2.weight', 'text.transformer.resblocks.3.ln_2.bias', 'text.transformer.resblocks.3.mlp.c_fc.weight', 'text.transformer.resblocks.3.mlp.c_fc.bias', 'text.transformer.resblocks.3.mlp.c_proj.weight', 'text.transformer.resblocks.3.mlp.c_proj.bias', 'text.transformer.resblocks.4.ln_1.weight', 'text.transformer.resblocks.4.ln_1.bias', 'text.transformer.resblocks.4.attn.in_proj_weight', 'text.transformer.resblocks.4.attn.in_proj_bias', 'text.transformer.resblocks.4.attn.out_proj.weight', 'text.transformer.resblocks.4.attn.out_proj.bias', 'text.transformer.resblocks.4.ln_2.weight', 'text.transformer.resblocks.4.ln_2.bias', 'text.transformer.resblocks.4.mlp.c_fc.weight', 'text.transformer.resblocks.4.mlp.c_fc.bias', 'text.transformer.resblocks.4.mlp.c_proj.weight', 'text.transformer.resblocks.4.mlp.c_proj.bias', 'text.transformer.resblocks.5.ln_1.weight', 'text.transformer.resblocks.5.ln_1.bias', 'text.transformer.resblocks.5.attn.in_proj_weight', 'text.transformer.resblocks.5.attn.in_proj_bias', 'text.transformer.resblocks.5.attn.out_proj.weight', 'text.transformer.resblocks.5.attn.out_proj.bias', 'text.transformer.resblocks.5.ln_2.weight', 'text.transformer.resblocks.5.ln_2.bias', 'text.transformer.resblocks.5.mlp.c_fc.weight', 'text.transformer.resblocks.5.mlp.c_fc.bias', 'text.transformer.resblocks.5.mlp.c_proj.weight', 'text.transformer.resblocks.5.mlp.c_proj.bias', 'text.transformer.resblocks.6.ln_1.weight', 'text.transformer.resblocks.6.ln_1.bias', 'text.transformer.resblocks.6.attn.in_proj_weight', 'text.transformer.resblocks.6.attn.in_proj_bias', 'text.transformer.resblocks.6.attn.out_proj.weight', 'text.transformer.resblocks.6.attn.out_proj.bias', 'text.transformer.resblocks.6.ln_2.weight', 'text.transformer.resblocks.6.ln_2.bias', 'text.transformer.resblocks.6.mlp.c_fc.weight', 'text.transformer.resblocks.6.mlp.c_fc.bias', 'text.transformer.resblocks.6.mlp.c_proj.weight', 'text.transformer.resblocks.6.mlp.c_proj.bias', 'text.transformer.resblocks.7.ln_1.weight', 'text.transformer.resblocks.7.ln_1.bias', 'text.transformer.resblocks.7.attn.in_proj_weight', 'text.transformer.resblocks.7.attn.in_proj_bias', 'text.transformer.resblocks.7.attn.out_proj.weight', 'text.transformer.resblocks.7.attn.out_proj.bias', 'text.transformer.resblocks.7.ln_2.weight', 'text.transformer.resblocks.7.ln_2.bias', 'text.transformer.resblocks.7.mlp.c_fc.weight', 'text.transformer.resblocks.7.mlp.c_fc.bias', 'text.transformer.resblocks.7.mlp.c_proj.weight', 'text.transformer.resblocks.7.mlp.c_proj.bias', 'text.transformer.resblocks.8.ln_1.weight', 'text.transformer.resblocks.8.ln_1.bias', 'text.transformer.resblocks.8.attn.in_proj_weight', 'text.transformer.resblocks.8.attn.in_proj_bias', 'text.transformer.resblocks.8.attn.out_proj.weight', 'text.transformer.resblocks.8.attn.out_proj.bias', 'text.transformer.resblocks.8.ln_2.weight', 'text.transformer.resblocks.8.ln_2.bias', 'text.transformer.resblocks.8.mlp.c_fc.weight', 'text.transformer.resblocks.8.mlp.c_fc.bias', 'text.transformer.resblocks.8.mlp.c_proj.weight', 'text.transformer.resblocks.8.mlp.c_proj.bias', 'text.transformer.resblocks.9.ln_1.weight', 'text.transformer.resblocks.9.ln_1.bias', 'text.transformer.resblocks.9.attn.in_proj_weight', 'text.transformer.resblocks.9.attn.in_proj_bias', 'text.transformer.resblocks.9.attn.out_proj.weight', 'text.transformer.resblocks.9.attn.out_proj.bias', 'text.transformer.resblocks.9.ln_2.weight', 'text.transformer.resblocks.9.ln_2.bias', 'text.transformer.resblocks.9.mlp.c_fc.weight', 'text.transformer.resblocks.9.mlp.c_fc.bias', 'text.transformer.resblocks.9.mlp.c_proj.weight', 'text.transformer.resblocks.9.mlp.c_proj.bias', 'text.transformer.resblocks.10.ln_1.weight', 'text.transformer.resblocks.10.ln_1.bias', 'text.transformer.resblocks.10.attn.in_proj_weight', 'text.transformer.resblocks.10.attn.in_proj_bias', 'text.transformer.resblocks.10.attn.out_proj.weight', 'text.transformer.resblocks.10.attn.out_proj.bias', 'text.transformer.resblocks.10.ln_2.weight', 'text.transformer.resblocks.10.ln_2.bias', 'text.transformer.resblocks.10.mlp.c_fc.weight', 'text.transformer.resblocks.10.mlp.c_fc.bias', 'text.transformer.resblocks.10.mlp.c_proj.weight', 'text.transformer.resblocks.10.mlp.c_proj.bias', 'text.transformer.resblocks.11.ln_1.weight', 'text.transformer.resblocks.11.ln_1.bias', 'text.transformer.resblocks.11.attn.in_proj_weight', 'text.transformer.resblocks.11.attn.in_proj_bias', 'text.transformer.resblocks.11.attn.out_proj.weight', 'text.transformer.resblocks.11.attn.out_proj.bias', 'text.transformer.resblocks.11.ln_2.weight', 'text.transformer.resblocks.11.ln_2.bias', 'text.transformer.resblocks.11.mlp.c_fc.weight', 'text.transformer.resblocks.11.mlp.c_fc.bias', 'text.transformer.resblocks.11.mlp.c_proj.weight', 'text.transformer.resblocks.11.mlp.c_proj.bias'])\n"
     ]
    }
   ],
   "source": [
    "from clip.eva_clip import create_model\n",
    "import torch\n",
    "\n",
    "model = create_model(\"EVA_AltCLIP_g_14\",\"/sharefs/baai-mrnd/sunquan/eva_clip_psz14.pt\")\n",
    "model.text.load_state_dict(torch.load(\"/home/chenzhongzhi/save_ckpt/kd_9lgs_18m_eva_debug/hf_encoder.pt\",map_location='cpu'))\n",
    "torch.save(model.state_dict(),\"/sharefs/baai-mrnd/sunquan/eva_clip_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model_p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "(model_p ) >> 20 # Million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from KDmodel checkpoint.\n",
    "from open_source_models.modeling_altclip import AltCLIP\n",
    "from open_source_models.processing_altclip import AltCLIPProcessor\n",
    "from open_source_models.modeling_kd import KDmodel\n",
    "from transformers import PretrainedConfig\n",
    "import torch\n",
    "model_path = '/home/chenzhongzhi/save_ckpt/9lg_mlm'\n",
    "\n",
    "config = PretrainedConfig.from_json_file(model_path + \"/config.json\")\n",
    "kd = KDmodel.from_pretrained(model_path,config=config)\n",
    "model = AltCLIP.from_pretrained(\"/home/chenzhongzhi/save_ckpt/xlm1024-9lg-18m_ft\")\n",
    "model_p = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_p ) >> 20 # Million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'logit_scale', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaSeriesModelWithTransformation were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['transformation.bias', 'lm_head.decoder.bias', 'transformation.weight', 'pre_LN.weight', 'pre_LN.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model from KDmodel checkpoint.\n",
    "from open_source_models.modeling_altclip import AltCLIP\n",
    "from open_source_models.processing_altclip import AltCLIPProcessor\n",
    "from open_source_models.modeling_kd import KDmodel\n",
    "from transformers import PretrainedConfig\n",
    "import torch\n",
    "model_path = '/home/chenzhongzhi/save_ckpt/9lg_mlm'\n",
    "\n",
    "config = PretrainedConfig.from_json_file(model_path + \"/config.json\")\n",
    "config.add_lm_task=True\n",
    "config.eva=False\n",
    "kd = KDmodel.from_pretrained(model_path,config=config)\n",
    "model = AltCLIP.from_pretrained(\"/home/chenzhongzhi/save_ckpt/xlm1024-9lg-18m_ft\")\n",
    "processor = AltCLIPProcessor.from_pretrained(\"/home/chenzhongzhi/save_ckpt/xlm1024-9lg-18m_ft\")\n",
    "model.text_model = kd.student\n",
    "model.config.text_config = model.text_model.config\n",
    "# model.load_state_dict(torch.load(model_path + \"/xlmr_9lg_clip_18m_ft1e.pt\",map_location='cpu'),strict=True)\n",
    "model.save_pretrained(\"/home/chenzhongzhi/save_ckpt/9lg_lmloss\")\n",
    "processor.save_pretrained(\"/home/chenzhongzhi/save_ckpt/9lg_lmloss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/chenzhongzhi/multi-clip/multi-clip/debug.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwudao.16-50g/home/chenzhongzhi/multi-clip/multi-clip/debug.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     image_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_image(image)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwudao.16-50g/home/chenzhongzhi/multi-clip/multi-clip/debug.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     text_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_text(text)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bwudao.16-50g/home/chenzhongzhi/multi-clip/multi-clip/debug.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     logits_per_image, logits_per_text \u001b[39m=\u001b[39m model(image, text)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwudao.16-50g/home/chenzhongzhi/multi-clip/multi-clip/debug.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     probs \u001b[39m=\u001b[39m logits_per_image\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bwudao.16-50g/home/chenzhongzhi/multi-clip/multi-clip/debug.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLabel probs:\u001b[39m\u001b[39m\"\u001b[39m, probs)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from clip import build_eva_model_and_transforms,tokenize\n",
    "from PIL import Image\n",
    "\n",
    "eva_clip_path = \"/sharefs/baai-mrnd/sunquan/eva_clip_psz14.pt\"\n",
    "model_name = \"EVA_CLIP_g_14\"\n",
    "image_path = \"clip/some_image.jpg\"\n",
    "caption = [\"a diagram\", \"a dog\", \"a cat\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = build_eva_model_and_transforms(model_name, pretrained=eva_clip_path)\n",
    "\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "text = tokenize(caption).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# extract text encoder from EVA\n",
    "from clip.eva_clip import create_model\n",
    "model = create_model(\"EVA_CLIP_g_14\",\"/sharefs/baai-mrnd/sunquan/eva_clip_psz14.pt\")\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# transformers version >= 4.21.0\n",
    "from modeling_altclip import AltCLIP\n",
    "from processing_altclip import AltCLIPProcessor\n",
    "\n",
    "# now our repo's in private, so we need `use_auth_token=True`\n",
    "model = AltCLIP.from_pretrained(\"BAAI/AltCLIP\")\n",
    "processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")\n",
    "processor.tokenizer([text...])\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.text,\"/home/chenzhongzhi/ckpt/EVA_text.pt\")\n",
    "# model.text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'psutil'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "def quickSort(left,right,nums):\n",
    "    # find the partition\n",
    "    i = left - 1\n",
    "    key = nums[right]\n",
    "    for j in range(left,right):\n",
    "        if nums[j] < key:\n",
    "            i += 1\n",
    "            nums[i],nums[j] = nums[j],nums[i]\n",
    "    idx = i+1 \n",
    "    nums[idx],nums[right] = nums[right],nums[idx]\n",
    "    quickSort(left,idx-1,nums)\n",
    "    quickSort(idx+1,right,nums)\n",
    "    \n",
    "    \n",
    "nums = [1,2,3,31,1,1,2,6,7]\n",
    "\n",
    "quickSort(0,len(nums),nums)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Feb 24 2021, 21:46:12) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
